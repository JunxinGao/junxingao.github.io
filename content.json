{"pages":[{"title":"About","text":"Stay hungry, stay foolish. 1234567891011&quot;&quot;&quot; _____/\\\\\\\\\\\\\\\\\\\\\\\\______/\\\\\\\\\\\\\\\\\\\\\\__/\\\\\\_______/\\\\\\_ ___/\\\\\\//////////______\\/////\\\\\\///__\\///\\\\\\___/\\\\\\/__ __/\\\\\\_____________________\\/\\\\\\_______\\///\\\\\\\\\\\\/____ _\\/\\\\\\____/\\\\\\\\\\\\\\_________\\/\\\\\\_________\\//\\\\\\\\______ _\\/\\\\\\___\\/////\\\\\\_________\\/\\\\\\__________\\/\\\\\\\\______ _\\/\\\\\\_______\\/\\\\\\_________\\/\\\\\\__________/\\\\\\\\\\\\_____ _\\/\\\\\\_______\\/\\\\\\__/\\\\\\___\\/\\\\\\________/\\\\\\////\\\\\\___ _\\//\\\\\\\\\\\\\\\\\\\\\\\\/__\\//\\\\\\\\\\\\\\\\\\_______/\\\\\\/___\\///\\\\\\_ __\\////////////_____\\/////////_______\\///_______\\///__&quot;&quot;&quot;","link":"/about/index.html"}],"posts":[{"title":"Hands-on Machine Learning with Scikit-Learn &amp; TensorFlow Study Notes—Chapter 1","text":"本文主要记录 Hands-on Machine Learning with Scikit-Learn &amp; TensorFlow 这本书第一章的学习笔记。 什么是机器学习?机器学习是通过编程让计算机从数据中进行学习的科学(和艺术)。 下面是一个更广义的概念: 机器学习是让计算机具有学习的能力,无需进行明确编程。——亚瑟·萨缪尔,1959 和一个工程性的概念: 计算机程序利用经验E学习任务T, 性能是P, 如果针对任务T的性能P随着经验E不断增长,则称为机器学习。——汤姆·米切尔,1997 机器学习系统的类型机器学习有多种类型,可以根据如下规则进行分类: 是否在人类监督下进行训练 (监督, 非监督, 半监督和强化学习) 是否可以动态渐进学习 (在线学习vs批量学习) 它们是否只是通过简单地比较新的数据点和已知的数据点, 或者在训练数据中进行模式识别, 以建立一个预测模型, 就像科学家所做的那样 (基于实例学习vs基于模型学习) 监督/非监督学习监督学习训练算法的训练数据包含了答案(称为label) 非监督学习训练算法的训练数据不包含label 半监督学习一些算法可以处理部分带标签的训练数据, 通常是大量不带标签数据加上小部分带标签数据。 强化学习学习系统在这里被称为智能体(agent), 可以对环境进行观察, 选择和执行动作, 获得奖励(负奖励是惩罚)。然后它必须自己学习哪个是最佳方法(称为策略, policy), 以得到长久的最大奖励。策略决定了智能体在给定情况下应该采取的行动。 批量和在线学习批量学习在批量学习中,系统不能进行持续学习: 必须用所有可用数据进行训练。这通常会占用大量时间和计算资源,所以一般是线下做的。首先是进行训练, 然后部署在生产环境且停止学习,它只是使用已经学到的策略。这称为离线学习。 在线学习在在线学习中,是用数据实例持续地进行训练, 可以一次一个或一次几个实例(称为小批量)。每个学习步骤都很快且廉价,所以系统可以动态地学习到达的新数据。 基于实例vs基于模型学习基于实例学习系统先用记忆学习案例,然后使用相似度测量推广到新的例子 基于模型学习另一种从样本集进行归纳的方法是建立这些样本的模型, 然后使用这个模型进行预测。","link":"/2019/01/07/Hands-on-Machine-Learning-with-Scikit-Learn-TensorFlow-Study-Notes%E2%80%94Chapter-1/"},{"title":"Hands-on Machine Learning with Scikit-Learn &amp; TensorFlow Study Notes—Chapter 2","text":"本文主要记录 Hands-on Machine Learning with Scikit-Learn &amp; TensorFlow 这本书第二章预测加州房价的学习笔记。 Overview本章中，我们会假装在加州一家房地产公司工作，要求预测加州地区的房价。 作为一个数据科学家，完整完成一个项目需要如下步骤: Look at the big picture/项目概述. Get the data/获取数据. Discover and visualize the data to gain insights/发现并可视化数据,发现规律. Prepare the data for Machine Learning algorithms/为机器学习算法准备数据. Select a model and train it/选择模型,进行训练. Fine-tune your model/微调模型. Present your solution/给出解决方案. Launch, monitor, and maintain your system/部署、监控、维护系统. 本文围绕上述步骤进行学习。 项目概述欢迎来到机器学习房地产公司!你的第一个任务是利用加州普查数据,建立一个加州房价模型。这个数据包含每个街区组的人口、收入中位数、房价中位数等指标。 街区组是美国调查局发布样本数据的最小地理单位(一个街区通常有600 到3000人)。我们将其简称为“街区”。 你的模型要利用这个数据进行学习,然后根据其它指标,预测任何街区的的房价中位数。 获取数据本章中使用的数据集是StatLib的加州房产价格数据集，这个数据集是基于1990年加州普查的数据。作者把数据放到了github上方便下载。 1234567891011121314151617181920import osimport tarfilefrom six.moves import urllibDOWNLOAD_ROOT = &quot;https://raw.githubusercontent.com/ageron/handson-ml/master/&quot;HOUSING_PATH = &quot;../res/housing&quot;HOUSING_URL = DOWNLOAD_ROOT + &quot;datasets/housing/housing.tgz&quot;def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): if not os.path.isdir(housing_path): os.mkdir(housing_path) csv_path = os.path.join(housing_path, &quot;housing.tgz&quot;) if os.path.exists(csv_path): print(&quot;[gjx] csv already exist, skip download&quot;) return tgz_path = os.path.join(housing_path, &quot;housing.tgz&quot;) urllib.request.urlretrieve(housing_url, tgz_path) housing_tgz = tarfile.open(tgz_path) housing_tgz.extractall(path=housing_path) housing_tgz.close() 这里我们根据书中代码写一个下载的函数，从https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz这里去下载 housing.tgz并解压。 这里用到了python的six模块，它是用来专门用来兼容 Python 2 和 Python 3 的库。 解压出来的文件是一个csv文件，这里通过python的pandas 库来读取。 12345import pandas as pddef load_housing_data(housing_path=HOUSING_PATH): csv_path = os.path.join(housing_path, &quot;housing.csv&quot;) return pd.read_csv(csv_path) pandas基于NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。Pandas 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的工具。 下面就可以测试了 123456fetch_housing_data()housing = load_housing_data()print(housing.head()) //自动打印数据集的前五行print(housing.info()) //快速查看数据的描述,特别是总行数、每个属性的类型和非空值的数量print(housing[&quot;ocean_proximity&quot;].value_counts()) //查看该项中都有哪些类别,每个类别中都包含有多少个print(housing.describe()) //展示了数值属性的概括 输出结果 123456789101112131415161718192021222324252627282930313233343536373839 longitude latitude ... median_house_value ocean_proximity0 -122.23 37.88 ... 452600.0 NEAR BAY1 -122.22 37.86 ... 358500.0 NEAR BAY2 -122.24 37.85 ... 352100.0 NEAR BAY3 -122.25 37.85 ... 341300.0 NEAR BAY4 -122.25 37.85 ... 342200.0 NEAR BAY[5 rows x 10 columns]&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 20640 entries, 0 to 20639Data columns (total 10 columns):longitude 20640 non-null float64latitude 20640 non-null float64housing_median_age 20640 non-null float64total_rooms 20640 non-null float64total_bedrooms 20433 non-null float64population 20640 non-null float64households 20640 non-null float64median_income 20640 non-null float64median_house_value 20640 non-null float64ocean_proximity 20640 non-null objectdtypes: float64(9), object(1)memory usage: 1.6+ MBNone&lt;1H OCEAN 9136INLAND 6551NEAR OCEAN 2658NEAR BAY 2290ISLAND 5Name: ocean_proximity, dtype: int64 longitude ... median_house_valuecount 20640.000000 ... 20640.000000mean -119.569704 ... 206855.816909std 2.003532 ... 115395.615874min -124.350000 ... 14999.00000025% -121.800000 ... 119600.00000050% -118.490000 ... 179700.00000075% -118.010000 ... 264725.000000max -114.310000 ... 500001.000000 可视化数据并观察规律TODO","link":"/2019/01/08/Hands-on-Machine-Learning-with-Scikit-Learn-TensorFlow-Study-Notes%E2%80%94Chapter-2/"},{"title":"Open3d 开发环境","text":"Open3d 是 Intel Visual Computing Lab 在2018年开源的3D点云数据处理库，提供c++ 和 python接口，使用方便。相对于PCL(Point Cloud Library)体积更小，API也更简洁，当然功能没有PCL全，不过Open3d中关于配准算法和三维场景重建系统比PCL有所改进。我主要使用Python + Open3d 来快速搭建开发环境。 1. Prepare1.1 ENVconda create --name open3d python=3.6 conda activate open3d conda install -c open3d-admin open3d","link":"/2019/01/04/Open3d-Develop-ENV/"},{"title":"Point set registration","text":"点云配准算法是将两组点云数据进行匹配，构建新的点云数据包含输入的两组以达到拼接的效果。 ReferenceWiki Point set registration https://en.wikipedia.org/wiki/Point_set_registration Open3d implementation PCL Registration Module http://www.pointclouds.org/documentation/tutorials/registration_api.php Open3D Registration Example http://www.open3d.org/docs/tutorial/ (ICP registration,Global registration, Fast global registration, Multiway registration) 1. ICP(Iterative Closest Point迭代最近点)算法 TODO","link":"/2019/01/04/Point-set-registration/"},{"title":"Python Kafka","text":"kafka是一个分布式消息队列。具有高性能、持久化、多副本备份、横向扩展能力。生产者往队列里写消息，消费者从队列里取消息进行业务逻辑。一般在架构设计中起到解耦、削峰、异步处理的作用。 Confluent kafka clientReferencehttps://anaconda.org/conda-forge/python-confluent-kafka https://docs.confluent.io/current/clients/confluent-kafka-python/ https://github.com/confluentinc/confluent-kafka-python https://kafka.apache.org/quickstart Envs anaconda 4.5.11 python 3.6 python-confluent-kafka 0.11.4 Prepare 建立新的python3环境 conda create --name confluent-kafka-env python=3.6 conda activate confluent-kafka-env 通过conda安装 confluent kafka conda install python-confluent-kafka 下载并启动kafka server wget http://mirrors.hust.edu.cn/apache/kafka/2.1.0/kafka_2.11-2.1.0.tgz &amp;&amp; tar -xzf kafka_2.11-2.1.0.tgz cd kafka_2.11-2.1.0 bin/zookeeper-server-start.sh config/zookeeper.properties bin/kafka-server-start.sh config/server.properties Testhttps://github.com/Murugar/ConfluentPythonKafka.git Producer.py 1234567891011121314from time import gmtime, strftime, sleepfrom confluent_kafka import Producerp = Producer({'bootstrap.servers': 'localhost:9092'})while True: now = strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, gmtime()) p.produce('test_topic', now.encode('utf-8')) p.flush() sleep(1)#p.produce('mytopic', 'Test Message')#p.flush() Consumer.py 1234567891011121314from confluent_kafka import Consumer, KafkaErrorc = Consumer({'bootstrap.servers': 'localhost:9092', 'group.id': 'mygroup', 'default.topic.config': {'auto.offset.reset': 'smallest'}})c.subscribe(['test_topic'])running = Truewhile running: msg = c.poll() if not msg.error(): print('Received message: %s' % msg.value().decode('utf-8')) elif msg.error().code() != KafkaError._PARTITION_EOF: print(msg.error()) running = Falsec.close() 运行python python Producer.py python Consumer.py consumer 可以接收到来自 producer 的msg","link":"/2019/01/04/Python-Kafka/"},{"title":"Open3d Reconstruction System 学习","text":"Open3d 中Reconstruction System输入一些经过RGBD对齐的Depth 和 RGB 图像，经过处理会输出一个3D场景，主要用来进行场景建模。 Overview官方文档中介绍到使用此系统主要有4个步骤。 Make fragments 从输入的数据中构建出片段 Register fragments 把构建的片段进行拼接配准 Refine registration 优化之前的片段配准 Integrate scene 生成RGBD的场景模型 Quick start上述4个步骤对应命令分别是 cd examples/Python/ReconstructionSystem/ python run_system.py [config_file] --make python run_system.py [config_file] --register python run_system.py [config_file] --refine python run_system.py [config_file] --integrate config_file 是一个json文件，用来保存数据集的路径、相机的内参、调优参数等 e.g.ReconstructionSystem/config/tutorial.json 1234567891011121314{ &quot;name&quot;: &quot;Open3D reconstruction tutorial http://open3d.org/docs/tutorial/ReconstructionSystem/system_overview.html&quot;, &quot;path_dataset&quot;: &quot;dataset/tutorial/&quot;, &quot;path_intrinsic&quot;: &quot;&quot;, &quot;max_depth&quot;: 3.0, &quot;voxel_size&quot;: 0.05, &quot;max_depth_diff&quot;: 0.07, &quot;preference_loop_closure_odometry&quot;: 0.1, &quot;preference_loop_closure_registration&quot;: 5.0, &quot;tsdf_cubic_size&quot;: 3.0, &quot;icp_method&quot;: &quot;color&quot;, &quot;global_registration&quot;: &quot;ransac&quot;, &quot;python_multi_threading&quot;: true} 准备数据集如果手上没有数据集的话可以去下载官方提供的样本 e.g.cd ReconstructionSystem/scripts &amp;&amp; ./download_tutorial.sh 脚本会从google drive下载数据，这里需要根据自己的网络环境进行操作。 Run system cd examples/Python/ReconstructionSystem/ python run_system.py config/tutorial.json --make python run_system.py config/tutorial.json --register python run_system.py config/tutorial.json --refine python run_system.py config/tutorial.json --integrate 查看结果生成的场景模型会放在数据集目录下的scene目录 tutorial 数据集的效果如下 Make fragments解析 首先会去读取json配置文件, 设置默认参数。 123456print(&quot;making fragments from RGBD sequence.&quot;)make_clean_folder(join(config[&quot;path_dataset&quot;], config[&quot;folder_fragment&quot;]))[color_files, depth_files] = get_rgbd_file_lists(config[&quot;path_dataset&quot;])n_files = len(color_files)n_fragments = int(math.ceil(float(n_files) / \\ config['n_frames_per_fragment'])) 这里的n_fragments是根据参数n_frames_per_fragment和文件数目获得的，e.g. 如果有400组color+depth的数据，n_frames_per_fragment 设置100，那么n_fragments就是4。 TODO","link":"/2019/01/04/open3d-reconstruction-system-notes/"},{"title":"删除无用docker镜像","text":"命令123docker ps -a | grep &quot;Exited&quot; | awk '{print $1 }'|xargs docker stopdocker ps -a | grep &quot;Exited&quot; | awk '{print $1 }'|xargs docker rmdocker images|grep none|awk '{print $3 }'|xargs docker rmi","link":"/2021/08/06/remove_useless_docker_images/"},{"title":"隐藏ubuntu用户登录","text":"在登录界面隐藏普通用户在/var/lib/AccountsService/users/文件夹下新建一个与用户名相同的文件,在文件中加上SystemAccount=true e.g. 例如隐藏用户名为test_user 1234567cat /var/lib/AccountsService/users/test_user[InputSource0]xkb=us[User]XSession=SystemAccount=true","link":"/2021/08/06/ubuntu_hide_user_login/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Scikit-Learn","slug":"Scikit-Learn","link":"/tags/Scikit-Learn/"},{"name":"TensorFlow","slug":"TensorFlow","link":"/tags/TensorFlow/"},{"name":"Open3d","slug":"Open3d","link":"/tags/Open3d/"},{"name":"Point set registration","slug":"Point-set-registration","link":"/tags/Point-set-registration/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"kafka","slug":"kafka","link":"/tags/kafka/"},{"name":"3D reconstruction","slug":"3D-reconstruction","link":"/tags/3D-reconstruction/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"ubuntu","slug":"ubuntu","link":"/tags/ubuntu/"}],"categories":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"Open3d","slug":"Open3d","link":"/categories/Open3d/"},{"name":"Wiki","slug":"Wiki","link":"/categories/Wiki/"},{"name":"kafka","slug":"kafka","link":"/categories/kafka/"}]}